#!/usr/bin/env python3
"""
å°† YAMNet TensorFlow æ¨¡å‹è½¬æ¢ä¸º Core ML æ ¼å¼
"""

import tensorflow as tf
import coremltools as ct
import numpy as np
import os

def convert_yamnet_to_coreml():
    """è½¬æ¢ YAMNet æ¨¡å‹ä¸º Core ML æ ¼å¼"""
    
    # æ¨¡å‹è·¯å¾„
    yamnet_model_path = "qinghe/Models/Audio/yamnet-tensorflow2-yamnet-v1"
    output_path = "qinghe/qinghe/YAMNet.mlmodel"
    
    print("ğŸ”„ å¼€å§‹è½¬æ¢ YAMNet æ¨¡å‹...")
    
    try:
        # åŠ è½½ TensorFlow æ¨¡å‹
        print("ğŸ“¥ åŠ è½½ TensorFlow æ¨¡å‹...")
        loaded_model = tf.saved_model.load(yamnet_model_path)

        # è·å–æ¨ç†å‡½æ•°
        infer = loaded_model.signatures['serving_default']

        # åˆ›å»ºç¤ºä¾‹è¾“å…¥ (YAMNet æœŸæœ› 16kHz éŸ³é¢‘)
        # é€šå¸¸æ˜¯ 0.975 ç§’çš„éŸ³é¢‘ (15600 ä¸ªæ ·æœ¬)
        example_input = np.random.randn(15600).astype(np.float32)

        print("ğŸ” æµ‹è¯•æ¨¡å‹æ¨ç†...")
        # æµ‹è¯•æ¨ç†
        output = infer(tf.constant(example_input))
        print(f"âœ… æ¨¡å‹è¾“å‡ºå½¢çŠ¶: {[k + ': ' + str(v.shape) for k, v in output.items()]}")

        # è½¬æ¢ä¸º Core ML
        print("ğŸ”„ è½¬æ¢ä¸º Core ML æ ¼å¼...")

        # ç›´æ¥ä½¿ç”¨ SavedModel è·¯å¾„è¿›è¡Œè½¬æ¢
        # YAMNet çš„è¾“å…¥åç§°æ˜¯ "waveform"
        coreml_model = ct.convert(
            yamnet_model_path,
            inputs=[ct.TensorType(shape=(15600,), dtype=np.float32, name="waveform")],
            source="tensorflow",
            convert_to="mlprogram",  # ä½¿ç”¨æ–°çš„ ML Program æ ¼å¼
            compute_precision=ct.precision.FLOAT16,  # ä½¿ç”¨ FP16 å‡å°æ¨¡å‹å¤§å°
        )
        
        # è®¾ç½®æ¨¡å‹å…ƒæ•°æ®
        coreml_model.short_description = "YAMNet audio classification model"
        coreml_model.author = "Google Research"
        coreml_model.license = "Apache 2.0"
        coreml_model.version = "1.0"
        
        # è®¾ç½®è¾“å…¥æè¿°
        coreml_model.input_description["waveform"] = "Audio waveform (16kHz, mono, 0.975s = 15600 samples)"
        
        # è®¾ç½®è¾“å‡ºæè¿°
        for output_name in coreml_model.output_description:
            if "scores" in output_name.lower() or "prediction" in output_name.lower():
                coreml_model.output_description[output_name] = "Classification scores for 521 audio classes"
            elif "embedding" in output_name.lower():
                coreml_model.output_description[output_name] = "Audio embedding features"
        
        # ä¿å­˜æ¨¡å‹
        print(f"ğŸ’¾ ä¿å­˜ Core ML æ¨¡å‹åˆ°: {output_path}")
        coreml_model.save(output_path)
        
        print("âœ… YAMNet æ¨¡å‹è½¬æ¢å®Œæˆï¼")
        
        # éªŒè¯è½¬æ¢åçš„æ¨¡å‹
        print("ğŸ” éªŒè¯è½¬æ¢åçš„æ¨¡å‹...")
        loaded_model = ct.models.MLModel(output_path)
        spec = loaded_model.get_spec()
        
        print("ğŸ“‹ æ¨¡å‹ä¿¡æ¯:")
        print(f"  è¾“å…¥: {[f'{inp.name}: {inp.type}' for inp in spec.description.input]}")
        print(f"  è¾“å‡º: {[f'{out.name}: {out.type}' for out in spec.description.output]}")
        
        return True
        
    except Exception as e:
        print(f"âŒ è½¬æ¢å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    success = convert_yamnet_to_coreml()
    if success:
        print("ğŸ‰ è½¬æ¢æˆåŠŸï¼ç°åœ¨å¯ä»¥åœ¨ iOS åº”ç”¨ä¸­ä½¿ç”¨ YAMNet æ¨¡å‹äº†ã€‚")
    else:
        print("ğŸ’¥ è½¬æ¢å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯ã€‚")
